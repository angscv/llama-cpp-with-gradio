# LLAMA-CPP-WITH-GRADIO ![img](https://img.shields.io/badge/Template-black?color=red)


![img](
    https://img.shields.io/badge/PYTHON-black?style=for-the-badge&logo=python
) ![img](
    https://img.shields.io/badge/DOCKER-black?style=for-the-badge&logo=docker
) ![img](https://img.shields.io/badge/llama_CPP-black?style=for-the-badge&logo=cplusplus
) 


![img](
    https://img.shields.io/badge/3.10-black?style=flat&logo=python&label=pyhton
) ![img](
https://img.shields.io/badge/MIT-green?style=flat&label=license
)


Этот проект является шаблонным для использование llama.cpp в стриме через интерфейс Gradio. Представлены 3 примера его использования:
- В режиме генерации ответов.
- выход JSON формата
- вызов функций через llama.cpp



<a href="docs/example.gif" target="_blank" style="border-radius:10px; display: block; text-align: center;">
  <img src="docs/example.gif" alt="Пример" style="width:80%; height:auto; border-radius:10px;">
</a>


## Table of Contents
- [Installation](#installation)
  - [Local Installation](#local-installation)
  - [Docker Installation](#docker-installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Tasks](#tasks)
- [Contribution](#contribution)

## Installation

### Local Installation

To install all dependencies and run the project locally, follow these steps:

1. **Create a virtual environment and activate it:**
    ```sh
    conda create -n fourm python=3.10 -y
    conda activate fourm
    pip install --upgrade pip  # enable PEP 660 support
    pip install -e .
    ```

2. **Install the required Python dependencies:**
    ```sh
    pip install -r requirements.txt
    ```

3. **Download the model:**
    Ensure you have `wget` installed. You can download the model using:
    ```sh
    wget -P src/models/ https://huggingface.co/IlyaGusev/saiga_llama3_8b_gguf/resolve/main/model-q4_K.gguf
    ```
    
    Or you can download any model in GGUF format and place it in the `src/models` directory. Don't forget to change the `MODEL_PATH` variable in the `.env` file to specify which model you want to use.

4. **Run the Gradio app:**
    Navigate to the `src` directory and run the application:
    ```sh
    python3 src/ text
    ```
    Также параметр text можно заменить на json или function.

### Docker Installation

To build and run a Docker container, follow these steps:

1. **Build the Docker image:**
    ```sh
    docker build -t llama-gradio .
    ```

    Ensure the server is bound to ```0.0.0.0``` to be accessible from outside the Docker container. Change ```server_name``` variable in ```src/gradio_app.py``` before building the docker image.

2. **Run the Docker container:**
    ```sh
    docker run -p 8000:8000 --name llama_gradio_container llama-gradio
    ```


## Usage

Once the server is running, open your web browser and navigate to `http://127.0.0.1:8000` to interact with the Gradio interface. You can input text and get responses generated by the LLAMA model in real-time.

## Project Structure

```yaml
LLAMA-CPP-WITH-GRADIO.
├── Dockerfile
├── assets
├── LICENSE
├── README.md
├── requirements.txt
├── src
│   ├── examples
│   │   ├── function_chat.py
│   │   ├── json_chat.py
│   │   └── text_chat.py
│   ├── __main__.py
│   ├── env.py
│   ├── llama_inference.py
│   └── utils.py
└── weights
    └── download_gguf.py
```

## Tasks

- [ ] Build docker container.
- [x] Add llama JSON output example.
- [ ] Add llama function usage example.

## Contribution

Feel free to open an issue or submit a pull request. Contributions are welcome!
